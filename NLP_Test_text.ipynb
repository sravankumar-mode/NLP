{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hjKZ9ZEc2_M8",
    "outputId": "73d247f2-c332-475d-fbcc-7cb9be6e1ffa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk==3.3\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/09/3b1755d528ad9156ee7243d52aa5cd2b809ef053a0f31b53d92853dd653a/nltk-3.3.0.zip (1.4MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4MB 7.2MB/s \n",
      "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk==3.3) (1.15.0)\n",
      "Building wheels for collected packages: nltk\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for nltk: filename=nltk-3.3-cp36-none-any.whl size=1394472 sha256=85e990a8e710d7d1fa69cdfee006db99ab9363da5067356b1e12b98ee383250d\n",
      "  Stored in directory: /root/.cache/pip/wheels/d1/ab/40/3bceea46922767e42986aef7606a600538ca80de6062dc266c\n",
      "Successfully built nltk\n",
      "Installing collected packages: nltk\n",
      "  Found existing installation: nltk 3.2.5\n",
      "    Uninstalling nltk-3.2.5:\n",
      "      Successfully uninstalled nltk-3.2.5\n",
      "Successfully installed nltk-3.3\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk==3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dJw7UsYe3AgM",
    "outputId": "d6c727bd-2733-4fd3-8e25-701577c64832"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y5Vo_qN-01aB"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import Word\n",
    "import re\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KpnaxealTPcD",
    "outputId": "67cd90ef-4758-45a4-b3a7-510d6300e67a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "webZVCpvkess"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('/content/drive/MyDrive/text_emotion.csv')\n",
    "data = data.drop('author', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "FVDdM8wvsSsk",
    "outputId": "177817b4-0248-49e2-b085-740dd2b8eded"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1956967341</td>\n",
       "      <td>empty</td>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1956967666</td>\n",
       "      <td>sadness</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1956967696</td>\n",
       "      <td>sadness</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1956967789</td>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1956968416</td>\n",
       "      <td>neutral</td>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tweet_id   sentiment                                            content\n",
       "0  1956967341       empty  @tiffanylue i know  i was listenin to bad habi...\n",
       "1  1956967666     sadness  Layin n bed with a headache  ughhhh...waitin o...\n",
       "2  1956967696     sadness                Funeral ceremony...gloomy friday...\n",
       "3  1956967789  enthusiasm               wants to hang out with friends SOON!\n",
       "4  1956968416     neutral  @dannycastillo We want to trade with someone w..."
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jr8KAxE1xSAr",
    "outputId": "df21d18a-9cd4-4d61-e351-1980df5dc699"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9IOXJpzatCCI"
   },
   "outputs": [],
   "source": [
    "# Dropping rows with other emotion labels\n",
    "data = data.drop(data[data.sentiment == 'anger'].index)\n",
    "data = data.drop(data[data.sentiment == 'boredom'].index)\n",
    "data = data.drop(data[data.sentiment == 'enthusiasm'].index)\n",
    "data = data.drop(data[data.sentiment == 'empty'].index)\n",
    "data = data.drop(data[data.sentiment == 'fun'].index)\n",
    "data = data.drop(data[data.sentiment == 'relief'].index)\n",
    "data = data.drop(data[data.sentiment == 'surprise'].index)\n",
    "data = data.drop(data[data.sentiment == 'love'].index)\n",
    "data = data.drop(data[data.sentiment == 'hate'].index)\n",
    "data = data.drop(data[data.sentiment == 'neutral'].index)\n",
    "data = data.drop(data[data.sentiment == 'worry'].index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "37HjQiaUhuXw",
    "outputId": "604808d0-aa11-4f4d-c1bb-3f5c018783a2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10374, 3)"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JdQ8JLlPtIRG"
   },
   "outputs": [],
   "source": [
    "#we have to take care of countless combinations, special characters, and not to mention, \n",
    "#the SMS lingo and slang for which even the dictionary can’t be used for reference.\n",
    "\n",
    "#First, let’s bring some uniformity to the text by making everything lowercase, removing \n",
    "##punctuation, and stop words (like prepositions).\n",
    "\n",
    "# Now convertibg all the letters to lower cases\n",
    "data['content'] = data['content'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "\n",
    "# Now Removing Punctuation and also Symbols\n",
    "data['content'] = data['content'].str.replace('[^\\w\\s]',' ')\n",
    "\n",
    "# Now Removing Stop Words- prepositions etc using NLTK\n",
    "stop = stopwords.words('english')\n",
    "data['content'] = data['content'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "\n",
    "\n",
    "#To gain any proper insight, we need to get all the words to their root form\n",
    "#i.e the variants of a word within the text (for example plural forms, past tense, etc) \n",
    "#must all be converted to the base word it represents.\n",
    "#This is called lemmatisation. \n",
    "#Lemmatisation code\n",
    "data['content'] = data['content'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "\n",
    "#After that, we added code to revert repetition of letters in a word with the assumption that hardly any word \n",
    "#has letters repeated more than twice, consecutively.\n",
    "#Though not very accurate, it can help in some corrections.\n",
    "#Correcting Letter Repetitions - which are repeated more than twice continuously\n",
    "def de_repeat(text):\n",
    "    pattern = re.compile(r\"(.)\\1{2,}\")\n",
    "    return pattern.sub(r\"\\1\\1\", text)\n",
    "\n",
    "data['content'] = data['content'].apply(lambda x: \" \".join(de_repeat(x) for x in x.split()))\n",
    "\n",
    "#Next we think that if a word is appearing only once in the entire sample of data\n",
    "#then it most likely has no influence in determining the sentiment of the text. \n",
    "#Hence we can remove all the rarely occurring words from the dataset \n",
    "#which are generally proper nouns and other insignificant words with respect to the current context.\n",
    "\n",
    "# Code to find the top 10,000 rarest words appearing in the data\n",
    "freq = pd.Series(' '.join(data['content']).split()).value_counts()[-10000:]\n",
    "\n",
    "# Removing all those rarely appearing words from the data\n",
    "freq = list(freq.index)\n",
    "data['content'] = data['content'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m8GcqeoptONG"
   },
   "outputs": [],
   "source": [
    "#Now the text data clean, precise, and error-free, each tweet is represented by a group of keywords. \n",
    "#Now, we need to perform ‘Feature Extraction’, i.e extracting some parameters from the data that can be presented numerically.\n",
    "#In this article, we consider two different features, TF-IDF & Count Vectors\n",
    "\n",
    "#We are changing the words - classes => 'sadness' as '1' & 'happiness' as '0'\n",
    "lbl_enc = preprocessing.LabelEncoder()\n",
    "y = lbl_enc.fit_transform(data.sentiment.values)\n",
    "\n",
    "# Splitting into training and testing data in 90:10 ratio\n",
    "X_train, X_val, y_train, y_val = train_test_split(data.content.values, y, stratify=y, random_state=42, test_size=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j-KsDL3_vPm6"
   },
   "outputs": [],
   "source": [
    "#This parameter gives the relative importance of a term in the data\n",
    "#is a measure of how frequently and rarely it appears in the text. \n",
    "#This can be directly extracted in python as follows\n",
    "\n",
    "# Extracting TF-IDF parameters here - \n",
    "tfidf = TfidfVectorizer(max_features=1000, analyzer='word',ngram_range=(1,3))\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_val_tfidf = tfidf.fit_transform(X_val)\n",
    "\n",
    "#This is another feature we consider and \n",
    "#as the name suggests we transform our tweet into an array having the count of appearances of each word in it.\n",
    "#The intuition here is that the text that conveys similar emotions may have the same words repeated over and over again.\n",
    "\n",
    "# Extracting Count Vectors Parameters\n",
    "count_vect = CountVectorizer(analyzer='word')\n",
    "count_vect.fit(data['content'])\n",
    "X_train_count =  count_vect.transform(X_train)\n",
    "X_val_count =  count_vect.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yfaGT18xsOW7",
    "outputId": "761c5cff-656d-4af2-ea7c-1ac5b2c5ddd6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive bayes tfidf accuracy 0.5597302504816956\n",
      "svm using tfidf accuracy 0.5789980732177264\n",
      "log reg tfidf accuracy 0.5789980732177264\n",
      "random forest tfidf accuracy 0.5549132947976878\n"
     ]
    }
   ],
   "source": [
    "# Model 1: Multinomial Naive Bayes Classifier\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_tfidf, y_train)\n",
    "y_pred = nb.predict(X_val_tfidf)\n",
    "print('naive bayes tfidf accuracy %s' % accuracy_score(y_pred, y_val))\n",
    "\n",
    "\n",
    "# Model 2: Linear SVM\n",
    "lsvm = SGDClassifier(alpha=0.001, random_state=5, max_iter=15, tol=None)\n",
    "lsvm.fit(X_train_tfidf, y_train)\n",
    "y_pred = lsvm.predict(X_val_tfidf)\n",
    "print('svm using tfidf accuracy %s' % accuracy_score(y_pred, y_val))\n",
    "\n",
    "\n",
    "# Model 3: logistic regression\n",
    "logreg = LogisticRegression(C=1)\n",
    "logreg.fit(X_train_tfidf, y_train)\n",
    "y_pred = logreg.predict(X_val_tfidf)\n",
    "print('log reg tfidf accuracy %s' % accuracy_score(y_pred, y_val))\n",
    "\n",
    "\n",
    "# Model 4: Random Forest Classifier\n",
    "rf = RandomForestClassifier(n_estimators=500)\n",
    "rf.fit(X_train_tfidf, y_train)\n",
    "y_pred = rf.predict(X_val_tfidf)\n",
    "print('random forest tfidf accuracy %s' % accuracy_score(y_pred, y_val))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s0RO8uVitg7f",
    "outputId": "00f81b2f-db35-433e-da69-b6711c0d72c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive bayes count vectors accuracy 0.7736030828516378\n",
      "lsvm using count vectors accuracy 0.7861271676300579\n",
      "log reg count vectors accuracy 0.7832369942196532\n",
      "random forest with count vectors accuracy 0.7620423892100193\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#The best model had an accuracy of just 54.43% (Logistic Regression) \n",
    "#which implies that our model is hardly classifying anything properly. \n",
    "#This is no good. This might be because of the complex nature of the textual dataset we are using.\n",
    "#so let’s build models using count vectors features\n",
    "## Building models using count vectors feature\n",
    "# Model 1: Multinomial Naive Bayes Classifier\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_count, y_train)\n",
    "y_pred = nb.predict(X_val_count)\n",
    "print('naive bayes count vectors accuracy %s' % accuracy_score(y_pred, y_val))\n",
    "\n",
    "\n",
    "# Model 2: Linear SVM\n",
    "lsvm = SGDClassifier(alpha=0.001, random_state=5, max_iter=15, tol=None)\n",
    "lsvm.fit(X_train_count, y_train)\n",
    "y_pred = lsvm.predict(X_val_count)\n",
    "print('lsvm using count vectors accuracy %s' % accuracy_score(y_pred, y_val))\n",
    "\n",
    "\n",
    "# Model 3: Logistic Regression\n",
    "logreg = LogisticRegression(C=1)\n",
    "logreg.fit(X_train_count, y_train)\n",
    "y_pred = logreg.predict(X_val_count)\n",
    "print('log reg count vectors accuracy %s' % accuracy_score(y_pred, y_val))\n",
    "\n",
    "\n",
    "# Model 4: Random Forest Classifier\n",
    "rf = RandomForestClassifier(n_estimators=500)\n",
    "rf.fit(X_train_count, y_train)\n",
    "y_pred = rf.predict(X_val_count)\n",
    "print('random forest with count vectors accuracy %s' % accuracy_score(y_pred, y_val))\n",
    "\n",
    "#now we can see that the accuracy is around 78.6\n",
    "#This might be because of the nature of this specific dataset where the emotion of the text is heavily dependent on the presence of some significant adjectives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e3JizPPO2rM3",
    "outputId": "b35474c2-5949-4dea-be60-b5118bd2bd28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting speechrecognition\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/26/e1/7f5678cd94ec1234269d23756dbdaa4c8cfaed973412f88ae8adf7893a50/SpeechRecognition-3.8.1-py2.py3-none-any.whl (32.8MB)\n",
      "\u001b[K     |████████████████████████████████| 32.8MB 119kB/s \n",
      "\u001b[?25hInstalling collected packages: speechrecognition\n",
      "Successfully installed speechrecognition-3.8.1\n"
     ]
    }
   ],
   "source": [
    "!pip install speechrecognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pU9JtsKe2-IC",
    "outputId": "0bff48c6-69de-4137-f299-9cb21ba5920d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pipwin\n",
      "  Downloading https://files.pythonhosted.org/packages/08/0f/a7df1770d2dcf99898aee562d6ce866e5dc78a5ccbf4ff25231ece4c99e8/pipwin-0.5.0-py2.py3-none-any.whl\n",
      "Collecting pyprind\n",
      "  Downloading https://files.pythonhosted.org/packages/1e/30/e76fb0c45da8aef49ea8d2a90d4e7a6877b45894c25f12fb961f009a891e/PyPrind-2.11.2-py3-none-any.whl\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from pipwin) (20.7)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pipwin) (2.23.0)\n",
      "Collecting js2py\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/6a/0385641ad1b52aae5c63820277a10e500c19e40fc4df5287f161aa287020/Js2Py-0.70-py3-none-any.whl (605kB)\n",
      "\u001b[K     |████████████████████████████████| 614kB 5.2MB/s \n",
      "\u001b[?25hCollecting pySmartDL>=1.3.1; python_version >= \"3.4\"\n",
      "  Downloading https://files.pythonhosted.org/packages/ac/6a/582286ea74c54363cba30413214767904f0a239e12253c3817feaf78453f/pySmartDL-1.3.4-py3-none-any.whl\n",
      "Requirement already satisfied: docopt in /usr/local/lib/python3.6/dist-packages (from pipwin) (0.6.2)\n",
      "Collecting beautifulsoup4>=4.9.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/41/e6495bd7d3781cee623ce23ea6ac73282a373088fcd0ddc809a047b18eae/beautifulsoup4-4.9.3-py3-none-any.whl (115kB)\n",
      "\u001b[K     |████████████████████████████████| 122kB 11.1MB/s \n",
      "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from pipwin) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->pipwin) (2.4.7)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pipwin) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pipwin) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pipwin) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pipwin) (2020.12.5)\n",
      "Collecting pyjsparser>=2.5.1\n",
      "  Downloading https://files.pythonhosted.org/packages/48/ef/c72abcfa2c6accd03e7c89c400790fc3d908c5804d50a7c4e9ceabd74d23/pyjsparser-2.7.1.tar.gz\n",
      "Requirement already satisfied: tzlocal>=1.2 in /usr/local/lib/python3.6/dist-packages (from js2py->pipwin) (1.5.1)\n",
      "Collecting soupsieve>1.2; python_version >= \"3.0\"\n",
      "  Downloading https://files.pythonhosted.org/packages/02/fb/1c65691a9aeb7bd6ac2aa505b84cb8b49ac29c976411c6ab3659425e045f/soupsieve-2.1-py3-none-any.whl\n",
      "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from tzlocal>=1.2->js2py->pipwin) (2018.9)\n",
      "Building wheels for collected packages: pyjsparser\n",
      "  Building wheel for pyjsparser (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pyjsparser: filename=pyjsparser-2.7.1-cp36-none-any.whl size=26001 sha256=49031ed86635a6ca1f9545341461023a2842eab69c81927cd2ed74743d926803\n",
      "  Stored in directory: /root/.cache/pip/wheels/a2/73/e6/3e433f3fd78257c3f971baf8cc9001cc0c4797268c61751e89\n",
      "Successfully built pyjsparser\n",
      "Installing collected packages: pyprind, pyjsparser, js2py, pySmartDL, soupsieve, beautifulsoup4, pipwin\n",
      "  Found existing installation: beautifulsoup4 4.6.3\n",
      "    Uninstalling beautifulsoup4-4.6.3:\n",
      "      Successfully uninstalled beautifulsoup4-4.6.3\n",
      "Successfully installed beautifulsoup4-4.9.3 js2py-0.70 pipwin-0.5.0 pySmartDL-1.3.4 pyjsparser-2.7.1 pyprind-2.11.2 soupsieve-2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pipwin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0S2xjDKr3BU4",
    "outputId": "6a642790-258a-439e-c8cd-4048fe2475c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "libasound2-dev is already the newest version (1.1.3-5ubuntu0.5).\n",
      "ffmpeg is already the newest version (7:3.4.8-0ubuntu0.2).\n",
      "Suggested packages:\n",
      "  portaudio19-doc\n",
      "The following NEW packages will be installed:\n",
      "  libportaudio2 libportaudiocpp0 portaudio19-dev\n",
      "0 upgraded, 3 newly installed, 0 to remove and 14 not upgraded.\n",
      "Need to get 184 kB of archives.\n",
      "After this operation, 891 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libportaudio2 amd64 19.6.0-1 [64.6 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libportaudiocpp0 amd64 19.6.0-1 [15.1 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu bionic/universe amd64 portaudio19-dev amd64 19.6.0-1 [104 kB]\n",
      "Fetched 184 kB in 1s (236 kB/s)\n",
      "Selecting previously unselected package libportaudio2:amd64.\n",
      "(Reading database ... 144865 files and directories currently installed.)\n",
      "Preparing to unpack .../libportaudio2_19.6.0-1_amd64.deb ...\n",
      "Unpacking libportaudio2:amd64 (19.6.0-1) ...\n",
      "Selecting previously unselected package libportaudiocpp0:amd64.\n",
      "Preparing to unpack .../libportaudiocpp0_19.6.0-1_amd64.deb ...\n",
      "Unpacking libportaudiocpp0:amd64 (19.6.0-1) ...\n",
      "Selecting previously unselected package portaudio19-dev:amd64.\n",
      "Preparing to unpack .../portaudio19-dev_19.6.0-1_amd64.deb ...\n",
      "Unpacking portaudio19-dev:amd64 (19.6.0-1) ...\n",
      "Setting up libportaudio2:amd64 (19.6.0-1) ...\n",
      "Setting up libportaudiocpp0:amd64 (19.6.0-1) ...\n",
      "Setting up portaudio19-dev:amd64 (19.6.0-1) ...\n",
      "Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n",
      "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!apt install libasound2-dev portaudio19-dev libportaudio2 libportaudiocpp0 ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4U4LwkcL2vPf",
    "outputId": "b106abc4-b240-4125-b9f2-dfcc9889e793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyaudio\n",
      "  Downloading https://files.pythonhosted.org/packages/ab/42/b4f04721c5c5bfc196ce156b3c768998ef8c0ae3654ed29ea5020c749a6b/PyAudio-0.2.11.tar.gz\n",
      "Building wheels for collected packages: pyaudio\n",
      "  Building wheel for pyaudio (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pyaudio: filename=PyAudio-0.2.11-cp36-cp36m-linux_x86_64.whl size=51612 sha256=5631798ffb71fb8455d527ed5b5fe42ca894fa83948c20768ccba48dcc67b1bf\n",
      "  Stored in directory: /root/.cache/pip/wheels/f4/a8/a4/292214166c2917890f85b2f72a8e5f13e1ffa527c4200dcede\n",
      "Successfully built pyaudio\n",
      "Installing collected packages: pyaudio\n",
      "Successfully installed pyaudio-0.2.11\n"
     ]
    }
   ],
   "source": [
    "!pip install pyaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q2ClGgcm2y26",
    "outputId": "25c89da1-5cb9-40ad-e797-3a2a0100ab0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyttsx3\n",
      "  Downloading https://files.pythonhosted.org/packages/33/9a/de4781245f5ad966646fd276259ef7cfd400ba3cf7d5db7c0d5aab310c20/pyttsx3-2.90-py3-none-any.whl\n",
      "Installing collected packages: pyttsx3\n",
      "Successfully installed pyttsx3-2.90\n"
     ]
    }
   ],
   "source": [
    "!pip install pyttsx3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lTCTfoKIUHBg"
   },
   "outputs": [],
   "source": [
    "import os,glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "id": "YueC7c_Z6bzq",
    "outputId": "ee563a20-6fe8-4d40-d412-4a070c580e7e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'import speech_recognition as sr\\nprint(sr.__version__)\\nr = sr.Recognizer()\\nfor file in glob.glob(\"/content/drive/MyDrive/happy-sad/*.wav\"):\\n        #file_name=os.path.basename(file)\\n        print(file)\\n        file_audio = sr.AudioFile(file)\\n        with file_audio as source:\\n          audio_text = r.record(source)\\n        print(type(audio_text))\\n        print(r.recognize_google(audio_text))'"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import speech_recognition as sr\n",
    "print(sr.__version__)\n",
    "r = sr.Recognizer()\n",
    "for file in glob.glob(\"/content/drive/MyDrive/happy-sad/*.wav\"):\n",
    "        #file_name=os.path.basename(file)\n",
    "        print(file)\n",
    "        file_audio = sr.AudioFile(file)\n",
    "        with file_audio as source:\n",
    "          audio_text = r.record(source)\n",
    "        print(type(audio_text))\n",
    "        print(r.recognize_google(audio_text))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tvwXxaRQfVDc",
    "outputId": "dad18dd6-e281-40cf-84f0-831c8733db9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.1\n",
      "<class 'speech_recognition.AudioData'>\n",
      "it is Christmas and latest all enjoy I am very excited about the kids there will be sweet and dance. Cakes will be wonderful but I am so sad that I can't see my friends I will miss them definitely I want to enjoy with my friend it will be a lot more fun in college\n"
     ]
    }
   ],
   "source": [
    "import speech_recognition as sr\n",
    "print(sr.__version__)\n",
    "r = sr.Recognizer()\n",
    "file_audio = sr.AudioFile(\"/content/drive/MyDrive/nlp22.wav\")\n",
    "with file_audio as source:\n",
    "  audio_text = r.record(source)\n",
    "print(type(audio_text))\n",
    "print(r.recognize_google(audio_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qt_a6nrwtKvU",
    "outputId": "d70ddaca-0a5b-4173-d27e-8c49fbb5fce0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "#Below are 8 random statements. The first 4 depict happiness. The last 4 depict sadness\n",
    "#happiness = 0 sadness=1\n",
    "tweets = pd.DataFrame(['It is Christmas and let us all enjoy', \n",
    "                       'I am very excited about the gifts', \n",
    "                       'There will be sweets and dance', \n",
    "                       'Oh, the cakes will be wonderful', \n",
    "                       'But I am so sad that I cant see my friends', \n",
    "                       'I will miss then definitely',\n",
    "                       'I want to enjoy with my friends',\n",
    "                       'It will be a lot more fun in college'])\n",
    "\n",
    "# Doing some preprocessing on these tweets as done before\n",
    "tweets[0] = tweets[0].str.replace('[^\\w\\s]',' ')\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "tweets[0] = tweets[0].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "from textblob import Word\n",
    "tweets[0] = tweets[0].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "\n",
    "# Extracting Count Vectors feature from our tweets\n",
    "tweet_count = count_vect.transform(tweets[0])\n",
    "\n",
    "#Predicting the emotion of the tweet using our already trained linear SVM\n",
    "tweet_pred = lsvm.predict(tweet_count)\n",
    "print(tweet_pred)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NLP-Test-text.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
